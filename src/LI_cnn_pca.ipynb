{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "PATH_X_TRAIN='./data/x_train.txt'\n",
    "PATH_Y_TRAIN='./data/y_train.txt'\n",
    "PATH_X_TEST='./data/x_test.txt'\n",
    "PATH_Y_TEST='./data/y_test.txt'\n",
    "\n",
    "#model \n",
    "INPUT_SIZE=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        0\n",
      "0       Klement Gottwaldi surnukeha palsameeriti ning ...\n",
      "1       Sebes, Joseph; Pereira Thomas (1961) (på eng)....\n",
      "2       भारतीय स्वातन्त्र्य आन्दोलन राष्ट्रीय एवम क्षे...\n",
      "3       Après lo cort periòde d'establiment a Basilèa,...\n",
      "4       ถนนเจริญกรุง (อักษรโรมัน: Thanon Charoen Krung...\n",
      "...                                                   ...\n",
      "117495  Nekoliko prašćića je rođeno na farmi Arableovi...\n",
      "117496  Tahiti of Otaheite is 'n eilandj in 't zuje va...\n",
      "117497  同年，太后崩。絳侯周勃、陳平諸臣共謀誅呂。朱虛侯章已殺呂產，文帝使人持節勞章。朱虛侯欲奪節信...\n",
      "117498  I det mindste opnåede Venedig den 18. april 14...\n",
      "117499  KR tók fljótlega að sér að bjóða uppá æfingar ...\n",
      "\n",
      "[117500 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#read dataset\n",
    "def read_txt(path):\n",
    "    handle = open(path, \"r\", encoding='utf-8')\n",
    "    df = pd.DataFrame(handle.readlines())\n",
    "    handle.close()\n",
    "    return df\n",
    "\n",
    "x_train=read_txt(PATH_X_TRAIN)\n",
    "y_train=read_txt(PATH_Y_TRAIN)\n",
    "x_test=read_txt(PATH_X_TEST)\n",
    "y_test=read_txt(PATH_Y_TEST)\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters(text):\n",
    "    new_text=''\n",
    "    for t in text:\n",
    "        if t.isalpha():\n",
    "            new_text+=t\n",
    "    return new_text\n",
    "\n",
    "texts_train=x_train.values\n",
    "texts_test=x_test.values\n",
    "texts_train=[s[0].replace('\\n','').lower() for s in texts_train]\n",
    "texts_test=[s[0].replace('\\n','').lower() for s in texts_test]\n",
    "\n",
    "for i in range(len(texts_train)):\n",
    "    texts_train[i]=letters(texts_train[i])\n",
    "for i in range(len(texts_test)):\n",
    "    texts_test[i]=letters(texts_test[i])\n",
    "    \n",
    "tokenizer=Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts preprocessing\n",
    "'''\n",
    "texts_train=x_train.values\n",
    "texts_test=x_test.values\n",
    "texts_train=[s[0].replace('\\n','').lower() for s in texts_train]\n",
    "texts_test=[s[0].replace('\\n','').lower() for s in texts_test]\n",
    "tokenizer=Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "'''\n",
    "#sequences\n",
    "texts_train=tokenizer.texts_to_sequences(texts_train)\n",
    "texts_test=tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "texts_train=pad_sequences(texts_train, maxlen=INPUT_SIZE, padding='post')\n",
    "texts_train=np.array(texts_train)\n",
    "texts_test=pad_sequences(texts_test, maxlen=INPUT_SIZE, padding='post')\n",
    "texts_test=np.array(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes preprocessing\n",
    "classes_train=y_train.values\n",
    "classes_test=y_test.values\n",
    "\n",
    "#dictionary for languages\n",
    "classes = np.unique(np.array(classes_train))\n",
    "nums=np.arange(len(classes))\n",
    "d = dict(zip(classes,nums))\n",
    "\n",
    "classes_train=[d[c[0]] for c in classes_train] \n",
    "classes_test=[d[c[0]] for c in classes_test] \n",
    "classes_train=to_categorical(classes_train)\n",
    "classes_test=to_categorical(classes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9627"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size=len(tokenizer.word_index)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings\n",
    "embeddings_weights=[]\n",
    "embeddings_weights.append(np.zeros(voc_size))\n",
    "\n",
    "\n",
    "for char, i in tokenizer.word_index.items():\n",
    "    onehot=np.zeros(voc_size)\n",
    "    onehot[i-1]=1\n",
    "    embeddings_weights.append(onehot)\n",
    "embeddings_weights=np.array(embeddings_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components=5000)\n",
    "#principalComponents = pca.fit_transform(embeddings_weights)\n",
    "#principalDf = pd.DataFrame(principalComponents)\n",
    "#print(principalDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'principalDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e5209c6ad654>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprincipalDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'principalDf' is not defined"
     ]
    }
   ],
   "source": [
    "#data=principalDf.values\n",
    "#data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "embeddings_weights=scipy.sparse.csr_matrix(embeddings_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#embedding_layer=Embedding(voc_size+1,\n",
    "                         #5000,\n",
    "                         #input_length=500,\n",
    "                         #weights=[data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(voc_size+1,\n",
    "                         voc_size,\n",
    "                         input_length=INPUT_SIZE,\n",
    "                         weights=[embeddings_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers=[[256, 7, 3],\n",
    "            [256, 7, 3],\n",
    "            [256, 7, 3],\n",
    "            [256, 3, -1],\n",
    "            [256, 3, -1],\n",
    "            [256, 3, -1],\n",
    "            [256, 3, 3]]\n",
    "\n",
    "\n",
    "#conv_layers=[[256, 7, 3]]\n",
    "\n",
    "fully_connected_layers=[1024, 1024]\n",
    "#fully_connected_layers=[512]   \n",
    "num_of_classes=len(classes)\n",
    "dropout=0.5\n",
    "optimizer='adam'\n",
    "loss='categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 9627)         92688756  \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 494, 256)          17251840  \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 494, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 164, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 158, 256)          459008    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 158, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 52, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 46, 256)           459008    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 46, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 13, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 11, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 9, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 235)               240875    \n",
      "=================================================================\n",
      "Total params: 113,461,855\n",
      "Trainable params: 113,461,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs=Input(shape=(INPUT_SIZE,), name='input', dtype='int64')\n",
    "x=embedding_layer(inputs)\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x=Conv1D(filter_num, filter_size)(x)\n",
    "    x=Activation('tanh')(x)\n",
    "    if pooling_size!=-1:\n",
    "        x=MaxPooling1D(pool_size=pooling_size)(x)\n",
    "x=Flatten()(x)\n",
    "for dense_size in fully_connected_layers:\n",
    "    x=Dense(dense_size, activation='softmax')(x)\n",
    "    x=Dropout(dropout)(x)\n",
    "predictions=Dense(num_of_classes, activation='softmax')(x)\n",
    "model=Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9628, 9627)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Annie\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 117500 samples, validate on 117500 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model.fit(texts_train, classes_train, \n",
    "        validation_data = (texts_test, classes_test),\n",
    "         batch_size=128,\n",
    "         epochs=10,\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
