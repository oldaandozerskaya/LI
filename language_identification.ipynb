{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.models import Sequential\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_Y_TRAIN = './wili-2018/y_train.txt'\n",
    "PATH_Y_TEST = './wili-2018/y_test.txt'\n",
    "NGRAMS_TRAIN = 'ng_freq_train.csv'\n",
    "NGRAMS_TEST = 'ng_freq_test.csv'\n",
    "\n",
    "SEED = 42\n",
    "LAYERS = [1000, 1000]\n",
    "DROPOUT = [0.2, 0.8]\n",
    "ACTIVATION = 'tanh'\n",
    "EARLY_STOPPING = 3\n",
    "MODEL_PATH = 'li_model.hdf5'\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path):#read y values\n",
    "    handle = open(path, \"r\", encoding='utf-8')\n",
    "    df = pd.DataFrame(handle.readlines())\n",
    "    handle.close()\n",
    "    return df.values\n",
    "\n",
    "def get_x_data():#read and transform x values\n",
    "    train_data=pd.read_csv(NGRAMS_TRAIN, sep=';')\n",
    "    test_data=pd.read_csv(NGRAMS_TEST, sep=';')\n",
    "    \n",
    "    scaler=MinMaxScaler()\n",
    "    train_data=pd.DataFrame(scaler.fit_transform(train_data))\n",
    "    test_data=pd.DataFrame(scaler.transform(test_data))\n",
    "    train_data, dev_data = split_train_dev(train_data)\n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "def get_y_data():\n",
    "    y_train=read_txt(PATH_Y_TRAIN)\n",
    "    y_train=y_train[1:]\n",
    "    y_test=read_txt(PATH_Y_TEST)\n",
    "    y_test=y_test[1:]\n",
    "    \n",
    "    classes = np.unique(np.array(y_train))\n",
    "    num_classes=len(classes)\n",
    "    nums=np.arange(num_classes)\n",
    "    d = dict(zip(classes,nums))\n",
    "\n",
    "    y_train=[d[c[0]] for c in y_train] \n",
    "    y_train = keras.utils.to_categorical(np.array(y_train), num_classes)\n",
    "    y_test=[d[c[0]] for c in y_test] \n",
    "    y_test = keras.utils.to_categorical(np.array(y_test), num_classes)\n",
    "    y_train, y_dev = split(train_dev(y_train))\n",
    "    return y_train, y_dev, y_test, num_classes\n",
    "\n",
    "def split_train_dev(data):\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(data)\n",
    "    train_length = int(0.9 * len(data))\n",
    "    data, dev_data = data[:train_length],\\\n",
    "        data[train_length:]\n",
    "    return data, dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "train_data, dev_data, test_data = get_x_data()\n",
    "y_train, y_dev, y_test, num_classes = get_y_data()\n",
    "\n",
    "print('Train x shape: {}, dev x shape: {}, test x shape: {}'.format(train_data.shape,\\\n",
    "    dev_data.shape, test_data.shape))\n",
    "print('Train y shape: {}, dev y shape: {}, test y shape: {}'.format(y_train.shape,\\\n",
    "    y_dev.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inp, num_classes)\n",
    "    model = Sequential()\n",
    "    for i, l in enumerate(LAYERS):\n",
    "        if i == 0:\n",
    "            model.add(Dense(l, input_dim=inp, activation=ACTIVATION))\n",
    "        else:\n",
    "            model.add(Dense(l, activation=ACTIVATION))\n",
    "        model.add(Dropout(DROPOUT[i]))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def train_model(model):\n",
    "    callbacks = []\n",
    "    if EARLY_STOPPING is not None:\n",
    "        callbacks.append(EarlyStopping(monitor=\"val_acc\", patience=EARLY_STOPPING))\n",
    "    model_checkpoint = ModelCheckpoint(filepath=MODEL_PATH, monitor=\"val_acc\",\n",
    "                                       save_best_only=True, save_weights_only=True)\n",
    "    callbacks.append(model_checkpoint)\n",
    "    model.fit(train_data, y_train, epochs=EPOCHS, verbose=1, validation_data=(dev_data, y_dev),\\\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(train_data.shape[1], num_classes)\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.argmax(model.predict(test_data), axis=1)\n",
    "answer = np.argmax(y_test, axis=1)\n",
    "print('F1-score: {}'.format(f1_score(predict, answer, average=\"macro\")*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
